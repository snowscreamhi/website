---
layout: post
title: Dimension Reduction
category: blog
permalink: /blogs/DimensionReduction
author:
  name: Quinn Hubbarth
  email: "qhubbar@clemson.edu"
---

Dimension reduction, also known as dimensionality reduction, is the process of reducing the amount of variables in a dataset to make the data easier to use. The resulting values from dimension reduction will be representations of the most important features in a dataset. Reduced dimensions are often more efficient in machine learning algorithms, although some data is lost when reducing dimensions. There are many different techniques and uses for dimension reduction; notably, dimension reduction is frequently used for data visualization.

Generally, the simplified datasets produced by dimension reduction are extremely effective for data visualization. For example, if a data point with 100 variables is reduced to just 2, it can be plotted on an 2D graph, with the dimensions set to X and Y. Although many variables are slimmed down using this technique of visualization, it is often easy to see patterns in the data.

This article will outline how dimension reduction can be effective for data visualization for a variety of contexts, and explain how different methods for dimension reduction can be vastly different.



# Datasets

To show how vastly applicable these techniques are, we chose three vastly different datasets. These datasets are frequently used as benchmarks for different machine learning algorithms. Additionally, they each are not purely numerical datasets. Each dataset had to be translated to purely numerical datasets through some form of data manipulation so that dimension reduction could be performed.

### AMES Housing
 * Home statistics
 * 81 columns of categorical and continuous data to describe a house
 * All continuous data and select categorical data used in reduction
 * Categorical data must be translated into numerical data through “dummy encoding”
 * Link to data [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)

### AG News
 * News articles
 * Each news article is categorized as world, sports, business, or sci/tech news.
 * To reduce the dimensions, the text must first be translated into a vector of 100 numbers using a natural language processing library known as FastText.
 * Because FastText is not a ground-truth representation of the article, these numbers are less accurate than the other two datasets
 * Link to data [here](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)

### MNIST
 * Hand-drawn digits
 * Images are translated to numerical data by assigning every column to a pixel in the image
 * 784 columns for pixels, 1 column for what the hand-drawn digit actually is
 * Link to data [here](https://pjreddie.com/projects/mnist-in-csv/)



# Methods

We used four different methods to test dimension reduction in these three datasets. The first two dimension reduction techniques we chose are PCA ([principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)) and SVD ([singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)). These two methods are over 100 years old in the realm of mathematics and statistics, but have been adapted for use in machine learning. They are relatively simple methods, and thus, can be performed with high speeds. Sci-kit learn offers libraries for both of these methods. 

The next two methods we chose were more complicated and computationally expensive. T-SNE (or [t-distributed stochastic neighbor embedding](http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)) was invented in 2008 and is primarily intended for visualization. It uses more complicated mathematics in its algorithms, such as manifold learning, and is thus more computationally expensive. UMAP ([uniform manifold approximation and projection](https://arxiv.org/abs/1802.03426)) is a technique invented in 2018 that expands on T-SNE. It is similarly effective in visualization, but also boasts success in general purpose reduction and runtime speed. Both TSNE and UMAP take significantly longer to run than PCA and SVD.



# Results


## AMES Housing

Note: the AMES Housing dataset originally has 81 features, 35 of which are continuous. When using all 81 features, dimension reduction largely fails. This is likely due to our process of “dummy encoding,” which expands single categorical columns to multiple numerical columns. To solve this, we only used 3 categorical variables which we thought would be significant: house style, kitchen quality, and utilities. When these columns were expanded during dummy encoding, the 3 columns turned into 16 columns. Combined with the 35 columns of continuous variables, the following graphs are representative of 51 columns of data reduced to 2 dimensions. We also decided to remove any house entries with prices larger than $350,000. This portion of the dataset frequently created outliers in our graphs, yet only represented 124 of the 2930 houses in the dataset. Removing them improved the graphs, but didn’t entirely eliminate outliers.

PCA
<div class="flourish-embed flourish-scatter" data-src="visualisation/1475111"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

SVD
<div class="flourish-embed flourish-scatter" data-src="visualisation/1475112"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

TSNE
<div class="flourish-embed flourish-scatter" data-src="visualisation/1474776"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

UMAP
<div class="flourish-embed flourish-scatter" data-src="visualisation/1475115"><script src="https://public.flourish.studio/resources/embed.js"></script></div>


## AG News

Note: the algorithm used to encode the text is based on a completely unsupervised learner. This means has no previous knowledge of the dataset; the encoder simply assigns a vector to a piece of text based on the semantics of the words. With supervised encodings, the graphs might be far more distinguishable.

PCA
<div class="flourish-embed flourish-scatter" data-src="visualisation/1475333"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

SVD
<div class="flourish-embed flourish-scatter" data-src="visualisation/1475358"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

TSNE
<div class="flourish-embed flourish-scatter" data-src="visualisation/1475362"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

UMAP
<div class="flourish-embed flourish-scatter" data-src="visualisation/1475365"><script src="https://public.flourish.studio/resources/embed.js"></script></div>


## MNIST

Note:
This dataset was provided with numerical representations of every pixel in the image. This made the dataset very easy to use. Also, the discrepancy between T-SNE/UMAP and PCA/SVD in this dataset was very noticeable.

PCA
<div class="flourish-embed flourish-scatter" data-src="visualisation/1516520"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

SVD
<div class="flourish-embed flourish-scatter" data-src="visualisation/1516585"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

TSNE
<div class="flourish-embed flourish-scatter" data-src="visualisation/1516593"><script src="https://public.flourish.studio/resources/embed.js"></script></div>

UMAP
<div class="flourish-embed flourish-scatter" data-src="visualisation/1516598"><script src="https://public.flourish.studio/resources/embed.js"></script></div>




# Concluding Remarks

Dimension reduction is extremely useful for visualization in a wide variety of machine learning applications. Different techniques for dimension reduction can produce very different results, however. 

More complex and modern algorithms seem to produce results that are easier to visualize. However, simple algorithms might be occasionally superior when it comes to simple linear prediction using the reduced dimension. For example, in the AMES Housing dataset, it would be quite easy to use a single line to predict house price based on the PCA and SVD reduced dimensions. Nonetheless, the complex algorithms like UMAP and TSNE seem to reign supreme in most regards otherwise.

Dimension reduction is an easy-to-use and practical solution for visualization of machine learning data. Using it to visualize results is an important part of many efforts in the realm of data science research.
